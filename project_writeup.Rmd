---
title: "Machine Learning Class Project"
author: "Derek Franks"
date: "June 20, 2015"
output: 
  html_document: 
    keep_md: yes
---
```{r echo=FALSE, message=FALSE, error=FALSE}
library(ggplot2)
library(tidyr)
library(plyr)
library(dplyr)
library(caret)
library(gbm)
library(doParallel)
registerDoParallel(cores = 4)
load("~/My Dropbox/Projects/machine_learning/gbmFit.rda")
```

#Introduction
The purpose of this data analysis is to use data collected from accellerometers located on the belt, forearm, arm, and dumbell of 6 participants to predict the type of exercise they were engaged in.

The training data set consists of 19622 observations of 160 variables, including a "target" variable, `classe` that identifies the type of exercise.  It is coded A through E.

The test data consists of 20 observations of 160 variables.

#Analysis
##Data Cleaning
I began the analysis by cleaning up the training data set.  A number of variables consisted largely of missing or NA data.  So I began by removing variables that contained NA values.  Also removed the `classe` variable, along with several other variables that were not likely to be predictive such as the record number, user name and timestamp data.

I then converted the remaining variables to numeric as a number had been incorrectly read in as factor or logical variables.  I then re-added the `classe` variable to the data set.

This left me with a training data set consisting of 19622 observations of 88 variables.

```{r}
setwd("C:/Users/Derek/Documents/My Dropbox/Projects/machine_learning")
train <- read.csv("pml-training.csv")
test <- read.csv("pml-testing.csv")

filteredTrain <- train[, !unlist(lapply(train, function(x) any(is.na(x))))]
filteredTrain <- filteredTrain[, -c(1:5,93)]
filteredTrain <- as.data.frame(lapply(filteredTrain, as.numeric))
filteredTrain <- cbind(classe = train$classe, filteredTrain)
dim(filteredTrain)
```

##Modeling
I decided to use a the `gbm` model.  This is a stochastic gradient boosted model that is quite robust for both classification and regression.  Because it is fundamentally a decision tree based model, it is also quite good at dealing with a large number of variables without having to engage in PCA or other variable selection techniques.

I used a standard 10-fold cross validation approach to optimize the model.  Additionally, I used a custom parameter tuning grid.  I checked the model at a 3, 5, and 7 level interaction depth, 50 through 250 trees, and both .01 and .1 shrinkage levels (essentially a "slow" and "fast" learner).

```{r}
fitControl <- trainControl(method = "cv",
                           number = 10)

gbmGrid <-  expand.grid(interaction.depth = c(3, 5, 7),
                        n.trees = (1:5)*50,
                        shrinkage = c(0.01, 0.1),
                        n.minobsinnode = 10)
```



```{r eval=FALSE}
set.seed(42)
gbmFit <- train(classe ~ ., data = filteredTrain,
                method = "gbm",
                trControl = fitControl,
                tuneGrid = gbmGrid,
                verbose = FALSE)
```

##Final model and error estimates
Optimal accuracy was achieved with 250 trees, an interaction depth of 7 and a shrinkage of 0.1.  This approach achieved accuracy of .9994904 on the training data.  This suggests that we're likely to see better than .99 out-of-sample accuracy given that the 10-fold cross-validation is still slightly biased towards the training data and gbm models, like many tree based models, can occassionally overfit the training data.

```{r echo=FALSE}
gbmFit
gbmFit$finalModel
plot(gbmFit)
# confusionMatrix(gbmFit)
confusionMatrix(gbmFit, norm = "average")

```

Additionally, we can look at the relative variable importance of the final model.  
```{r}
varImp(gbmFit)
```




#Test data
Some minimal cleaning was required to prepare the test data set.  Essentially, all of the variables had to be converted to numeric values and NA values were replaced with 0.

Once that was done, as expected, the `gbm` model exhibited high out-of-sample accuracy, correctly predicting all 20 test observations.

```{r}
numericTest <- as.data.frame(lapply(test, as.numeric))
numericTest[is.na(numericTest)] <- 0

```